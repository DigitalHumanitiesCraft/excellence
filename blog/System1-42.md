---
layout: post
title: "System 1.42: Wie (Frontier-)LLMs â€œtatsÃ¤chlichâ€ funktionieren"
author: "Christopher Pollin"
date: 2025-07-01
published: true

# Spezifische Metadaten fÃ¼r diesen Post
citation:
  type: "blog-post"
  container-title: "Digital Humanities Craft"
  URL: "https://dhcraft.org/excellence/blog/System1-42"
  language: "de"
  abstract: "Frontier-LLMs folgen weder Kahnemans schnellem System 1 noch dem analytischen System 2, sondern operieren als hybrides System 1.42 â€“ so lautet zumindest meine These, mit der ich die Diskussion anregen mÃ¶chte. Diese â€œhalluzinierenden Reasonerâ€ erzeugen plausible Antworten, ohne eine verlÃ¤ssliche SelbstprÃ¼fung durchzufÃ¼hren. Mit zunehmender Skalierung (mehr Daten, mehr Rechenzeit, mehr Token) halluzinieren sie zwar immer hÃ¤ufiger korrekte Antworten, jedoch nicht fÃ¼r alle Probleme und auch nicht immer. Ist das alles nur eine (fÃ¼r manche) Ã¼berzeugende Simulation? Oder emergiere hier tatsÃ¤chlich Generalisierung und damit echtes Reasoning? Die â€œ42â€-Referenz symbolisiert diese fundamentale Ungewissheit Ã¼ber etwas qualitativ Neues, fÃ¼r das es keine einfachen Antworten gibt. Der Text erklÃ¤rt die technischen Grundlagen und spannt den Bogen von LLM als Autocomplete Ã¼ber Emergenz und Grokking bis zu Reasoning. Wie gehen wir mit Systemen um, die funktional so tun, als ob sie verstehen, und die gleichzeitig ziemlich andersartig sind als wir? Der Text ist der Versuch, all das mit zahlreichen Referenzen zu kontextualisieren."
  
dublin_core:
  creator: "Christopher Pollin"
  publisher: "Digital Humanities Craft"
  subject: ["Applied Generative AI", "LLM", "Reasoning"]
  description: "Frontier-LLMs folgen weder Kahnemans schnellem System 1 noch dem analytischen System 2, sondern operieren als hybrides System 1.42 â€“ so lautet zumindest meine These, mit der ich die Diskussion anregen mÃ¶chte. Diese â€œhalluzinierenden Reasonerâ€ erzeugen plausible Antworten, ohne eine verlÃ¤ssliche SelbstprÃ¼fung durchzufÃ¼hren. Mit zunehmender Skalierung (mehr Daten, mehr Rechenzeit, mehr Token) halluzinieren sie zwar immer hÃ¤ufiger korrekte Antworten, jedoch nicht fÃ¼r alle Probleme und auch nicht immer. Ist das alles nur eine (fÃ¼r manche) Ã¼berzeugende Simulation? Oder emergiere hier tatsÃ¤chlich Generalisierung und damit echtes Reasoning? Die â€œ42â€-Referenz symbolisiert diese fundamentale Ungewissheit Ã¼ber etwas qualitativ Neues, fÃ¼r das es keine einfachen Antworten gibt. Der Text erklÃ¤rt die technischen Grundlagen und spannt den Bogen von LLM als Autocomplete Ã¼ber Emergenz und Grokking bis zu Reasoning. Wie gehen wir mit Systemen um, die funktional so tun, als ob sie verstehen, und die gleichzeitig ziemlich andersartig sind als wir? Der Text ist der Versuch, all das mit zahlreichen Referenzen zu kontextualisieren."
  type: "Blogpost"
  format: "text/html"
  rights: "CC BY 4.0"
  language: "de"

coins_data:
  rft_type: "blogPost"  # Standard COinS Format

website_title: "Digital Humanities Craft"
website_type: "Blog"
short_title: "System 1.42: Wie (Frontier-)LLMs â€œtatsÃ¤chlichâ€ funktionieren"
abstract: "Frontier-LLMs folgen weder Kahnemans schnellem System 1 noch dem analytischen System 2, sondern operieren als hybrides System 1.42 â€“ so lautet zumindest meine These, mit der ich die Diskussion anregen mÃ¶chte. Diese â€œhalluzinierenden Reasonerâ€ erzeugen plausible Antworten, ohne eine verlÃ¤ssliche SelbstprÃ¼fung durchzufÃ¼hren. Mit zunehmender Skalierung (mehr Daten, mehr Rechenzeit, mehr Token) halluzinieren sie zwar immer hÃ¤ufiger korrekte Antworten, jedoch nicht fÃ¼r alle Probleme und auch nicht immer. Ist das alles nur eine (fÃ¼r manche) Ã¼berzeugende Simulation? Oder emergiere hier tatsÃ¤chlich Generalisierung und damit echtes Reasoning? Die â€œ42â€-Referenz symbolisiert diese fundamentale Ungewissheit Ã¼ber etwas qualitativ Neues, fÃ¼r das es keine einfachen Antworten gibt. Der Text erklÃ¤rt die technischen Grundlagen und spannt den Bogen von LLM als Autocomplete Ã¼ber Emergenz und Grokking bis zu Reasoning. Wie gehen wir mit Systemen um, die funktional so tun, als ob sie verstehen, und die gleichzeitig ziemlich andersartig sind als wir? Der Text ist der Versuch, all das mit zahlreichen Referenzen zu kontextualisieren."

schema_type: "BlogPosting"
keywords: ["Applied Generative AI", "LLM", "Reasoning"]
---

![][image1]

Large Language Models (LLMs) liefern erstaunlich kohÃ¤rente Antworten, und es herrscht weiterhin Uneinigkeit darÃ¼ber, ob sie wirklich verstehen oder lediglich Muster reproduzieren, oder ob ihr scheinbares Verstehen aus bloÃŸer Musterreproduktion emergiert. Die ***Stochastic Parrots***[^1]\-Metapher charakterisiert sie als bloÃŸe Mustererkenner. Doch die neueste Generation dieser Modelle â€“ Frontier-LLMs wie OpenAIs *o3*, Googles *Gemini 2.5 Pro* oder Anthropics *Claude 4 Opus und Sonnet* â€“ stellt diese Sichtweise in Frage und entfacht die immer wieder aufflammende Diskussion, ob **echte Reasoning-FÃ¤higkeiten** entstehen.[^2] Handelt es sich vielleicht um eine emergente Eigenschaft ausreichend hochskalierter[^3] Modelle, oder ist es eine **Illusion von Reasoning**[^4], die aus Mustererkennung und Extrapolation gigantischer Trainingsdatenmengen entsteht? Wie funktionieren LLMs â€œtatsÃ¤chlichâ€?[^5]

Wir wissen es nicht. Wir wissen nicht genau, warum die Modelle bei manchen Aufgaben so gut funktionieren. Wie sie funktionieren, wissen wir natÃ¼rlich. Was wir beobachten: LLMs werden immer besser darin, korrekte Antworten zu â€œhalluzinierenâ€, ohne dass wir vorhersagen kÃ¶nnen, wann sie richtig liegen und wann nicht. Diese Beobachtung bezieht sich auf Frontier-Modelle, die bei unterschiedlichen Problemtypen unterschiedlich abschneiden.[^6] Aus den stochastischen Papageien sind 2025 ***Agentic Systems***[^7] geworden, die auf Reasoning-Modellen basieren und dadurch manche ihrer eigenen SchwÃ¤chen ausgleichen kÃ¶nnen. Ein LLM kann zwar nicht zÃ¤hlen, programmiert sich aber einen Taschenrechner und nutzt diesen per ***Tool Use*****[^8]**. Falsche AuskÃ¼nfte kann es durch Websuchen korrigieren, wobei es auch dabei halluzinieren kann. Wir mÃ¼ssen LLMs anders denken. Als etwas Neues. Ich schlage vor, Frontier-Modelle als ***System 1.42*** zu verstehen, angelehnt an Daniel Kahnemans[^9] Theorie der zwei Denksysteme:

**System 1** arbeitet schnell, intuitiv und automatisch. Die Frage â€œWas ist 2 \+ 2?â€ kann ohne mentale ReprÃ¤sentation beantwortet werden. **System 2** arbeitet langsamer, analytisch und bewusst. FÃ¼r die Frage â€œWas ist 439 Ã— 238?â€ mÃ¼ssen wir uns in irgendeiner Form bewusst (\!) im Kopf Gedanken darÃ¼ber machen und kÃ¶nnen erst dann eine korrekte Antwort liefern. Man kann sich aber natÃ¼rlich auch verrechnen.

LLMs operieren wie System 1, produzieren aber manchmal System-2-Ã¤hnliche KomplexitÃ¤t, ohne diese wirklich validieren zu kÃ¶nnen. â€œNie wirklichâ€ deswegen, weil es schon eine Form von Verifikation Ã¼ber das Erzeugen von Text gibt, der ein Problem weiter kontextualisiert. Neben â€œ*Letâ€™s think step by step*â€ ist â€œ*Verify step by step*â€ das Schlagwort.[^10] Christopher Summerfield beschreibt sie als *Strange New Minds*[^11], deren Outputs sich funktional kaum von echtem Denken unterscheiden lassen. Dies ergÃ¤nzt meine Charakterisierung von LLMs als ***halluzinierende Reasoner***, also Systeme, die aufgrund statistischer Mustererkennung scheinbar logische Schlussfolgerungen ziehen, jedoch ohne diese jemals tatsÃ¤chlich zu validieren. Da sich diese Eigenschaft wahrscheinlich auch in naher Zukunft nicht Ã¤ndern wird â€“ dazu brÃ¤uchte es eine andere Architektur[^12] â€“ steht die externe Validierung der erzeugten Token im Mittelpunkt. Gleichzeitig entstehen daraus nicht nur technische Herausforderungen, sondern auch tiefgreifende gesellschaftliche und psychologische Implikationen. Wie Summerfield betont, kÃ¶nnten Menschen zunehmend ihre AuthentizitÃ¤t und Kontrolle verlieren, wenn sie sich unkritisch auf KI-generierte Inhalte verlassen. Deren vermeintliche Logik existiert letztlich nÃ¤mlich nur oberflÃ¤chlich. Wenn man jedoch weiÃŸ, welche Fragen man wie stellen muss, kritisch bleibt, und dann in der Lage ist, herauszufinden, ob die Antwort korrekt ist, dann hat man ein sehr praktisches Werkzeug gefunden.

In Douglas Adams *Per Anhalter durch die Galaxis* liefert der Supercomputer *Deep Thought* nach Millionen Jahren die Antwort â€œ42â€. Diese Antwort ist bedeutungslos ohne die richtige Frage. LLMs operieren genau umgekehrt und generieren auf jede Frage Antworten, ohne deren Korrektheit Ã¼berprÃ¼fen zu kÃ¶nnen. Die â€œ42â€ in meiner System-1.42-Metapher verweist auf diese Ungewissheit. Auf etwas, das funktioniert und zugleich nicht funktioniert. Etwas dazwischen, das wir scheinbar noch nicht richtig benennen und fassen kÃ¶nnen.

Da ich kein Experte fÃ¼r maschinelles Lernen bin, ist meine Darstellung zwangslÃ¤ufig vereinfacht. Mir geht es darum, ein GespÃ¼r fÃ¼r die Funktionsweise von Frontier-Modellen zu vermitteln. Dieses VerstÃ¤ndnis ist nicht nur beim Prompt Engineering hilfreich, sondern hilft uns auch, uns auf die kommenden und bereits vorhandenen, sehr einschneidenden VerÃ¤nderungen einzustimmen und vorzubereiten.

# **Token, Embeddings, Context Window und Training**

Es ist notwendig, ein paar grundlegende Konzepte zu klÃ¤ren. Token sind atomare Verarbeitungseinheiten, Embeddings mathematische BedeutungsreprÃ¤sentationen, das Context Window das â€œArbeitsgedÃ¤chtnisâ€ und der Trainingsprozess prÃ¤gt die grundlegende Funktionsweise. Diese vier Konzepte bilden das technische Fundament fÃ¼r das VerstÃ¤ndnis von LLMs. Die Mathematik dahinter mÃ¼ssen Sie nicht beherrschen, schaden wÃ¼rde es aber natÃ¼rlich nicht.

## **Token**

Token bilden die grundlegende Einheit, in die Texte zerlegt werden, um von einem LLM verarbeitet zu werden. Ein Token kann ein ganzes Wort, ein Teil eines Wortes, Leer- oder Steuerzeichen oder gar nur ein einzelnes Zeichen sein. Diese Zerlegung erfolgt durch einen sogenannten **Tokenizer**[^13], ein Programm, das fÃ¼r jede Modellfamilie und Sprache optimiert wird.[^14] Der Satz â€œChristopher codes and uses XSL-T. The cat sat on the keyboard\!â€ besteht aus 11 WÃ¶rtern und 62 Zeichen (inklusive Leerzeichen). Nach der Tokenisierung entstehen fÃ¼r den Tokenizer fÃ¼r GPT4o von OpenAI daraus 15 Token:

![][image2]  
OpenAI Tokenizer. [https://platform.openai.com/tokenizer](https://platform.openai.com/tokenizer)

Token entsprechen nicht unserer intuitiven Wortwahrnehmung. Der technische Begriff â€œXSL-Tâ€ wird in drei separate Token (â€œXSLâ€, â€œ-â€ und â€œTâ€) zerlegt. Solche Fragmentierungen haben Konsequenzen. Jedes Token â€œbeansprucht Aufmerksamkeitâ€ im *Attention*\-Mechanismus der Transformer-Architektur. Zwei weitere Begriffe hier mal als Teaser angekÃ¼ndigt und spÃ¤ter erklÃ¤rt. Token mit wenig semantischem Gehalt, wie beispielsweise einzelne Bindestriche, tragen weniger dazu bei, eine â€œguteâ€ Antwort auf einen Prompt zu erhalten. Der Attention-Mechanismus erkennt ihre geringe Bedeutung und gewichtet sie entsprechend niedriger. Sie lenken das Modell also nicht ab, sondern blockieren vielmehr Platz im Context Window. Das ist vereinfacht, aber es hilft vielleicht als GedankenstÃ¼tze beim Arbeiten. Bei vereinzeltem Auftreten ist das unproblematisch. GroÃŸe Mengen Token â€“ etwa XML-basierte Formate wie Excel- oder PowerPoint-Dateien â€“ kÃ¶nnen das limitierte Kontextfenster schnell erschÃ¶pfen und die QualitÃ¤t der Antworten dadurch beeintrÃ¤chtigen. Die Effizienz hÃ¤ngt dabei vom jeweiligen Modell, Tokenizer und der GrÃ¶ÃŸe des Kontextfensters ab. Deshalb ist ein â€œGespÃ¼râ€ fÃ¼r den Token-Verbrauch wichtig. Ein Text im steirischen Dialekt mit Tippfehlern wird beispielsweise schlechter tokenisiert als ein Text mit fast dem gleichen Informationsgehalt, der im standardisierten, modernen Englisch reprÃ¤sentiert wird.

## **Embedding**

In einem LLM wird jedes Token als Embedding reprÃ¤sentiert. Ein Embedding ist eine Reihe von Zahlen, die einen Pfeil in einem abstrakten, mathematischen Raum mit Tausenden von Dimensionen definieren. Technisch gesprochen ist es ein Vektor. Dabei ermÃ¶glichen Embeddings mathematische Operationen wie Ã„hnlichkeitsberechnungen, wodurch semantische NÃ¤he quantitativ erfasst werden kann. Die Embeddings fÃ¼r die Token â€œKatzeâ€ und â€œHundâ€ liegen in diesem Raum nÃ¤her beieinander als â€œKatzeâ€ und â€œSteinâ€, da sie in Ã¤hnlichen Kontexten auftreten: Sie sind beide Haustiere, lebendig und man kann mit ihnen gut kuscheln. Mit Steinen kuschelt es sich hingegen nicht so gut. Dies ist ein bewusst etwas absurdes Beispiel, das so (vielleicht) nie explizit in den Trainingsdaten vorkommt. Das Modell kann solche ZusammenhÃ¤nge implizit aus Milliarden von Kontexten ableiten. Nicht der einzelne Vektor allein reprÃ¤sentiert die Bedeutung, sondern seine Lage relativ zu allen anderen Vektoren. Man kann sich den Raum wie ein riesiges Universum vorstellen. So mache ich das zumindest. Jedes Wort erscheint zunÃ¤chst als Ausgangsvektor. Die Transformer-Schichten transformieren jeden Token schrittweise, wodurch eine Flugbahn durch dieses Universum entsteht. Diese Transformation ist deterministisch. Gleiche Token-Sequenzen erzeugen gleiche Embeddings. Der Output kann trotzdem variieren durch Temperature-Sampling bei der Token-Auswahl. Dabei nÃ¤hern sich â€œKatzeâ€ und â€œHundâ€ dem Verb â€œkuschelnâ€, wÃ¤hrend â€œSteinâ€ auf Abstand bleibt und andere BedeutungsrÃ¤ume, je nach Kontext, durchquert. So wird aus dem festen Basis-Embedding Schritt fÃ¼r Schritt ein kontextabhÃ¤ngiger Vektor berechnet. Durch diese KontextabhÃ¤ngigkeit entsteht bei jeder Eingabe eine ganze Kette neuer, kontextualisierter Embeddings.

![][image3]  
Illustration eines Embedding-Raums, der zeigt, dass die Vektoren fÃ¼r â€šHundâ€˜ (dog) und â€šKatzeâ€˜ (cat) nah am Konzept â€škuschelnâ€˜ (cuddle) liegen, wÃ¤hrend der Vektor fÃ¼r â€šSteinâ€˜ (stone) deutlich weiter entfernt ist. Eigene Darstellung (Created with ChatGPT).

Diese Organisation emergiert durch das Training. Die Position jedes Token passt sich kontinuierlich an seine Beziehungen zu allen anderen Token an. So reprÃ¤sentiert das Modell implizit, dass â€œKÃ¶nigâ€ zu â€œKÃ¶niginâ€ in Ã¤hnlicher Relation steht wie â€œMannâ€ zu â€œFrauâ€, ohne dass es eine explizite Regeldefinition gibt. Jedoch bedeutet dies nicht automatisch, dass das Modell diese Relationen auch umgekehrt zuverlÃ¤ssig herstellen kann. WÃ¤hrend das Modell auf die Frage â€œWer ist die Mutter von Tom Cruise?â€ antworten kann, scheitert es mÃ¶glicherweise an der umgekehrten Relation â€œWer ist der Sohn von Mary Lee Pfeiffer?â€, falls diese Verbindung nicht explizit gelernt wurde. Zumindest war dies noch ein Problem im Jahr 2024.[^15] Diese dynamische BedeutungsreprÃ¤sentation erklÃ¤rt, warum LLMs kontextuell passende Fortsetzungen generieren kÃ¶nnen und warum dabei Fehler entstehen. Ihr â€œVerstehenâ€œ ist grundlegend anders als das menschliche. Es gilt allerdings nicht zwangslÃ¤ufig, dass Modelle, die ein scheinbar einfaches Problem A lÃ¶sen, ein komplexeres Problem B automatisch ebenfalls lÃ¶sen â€“ oder umgekehrt.

![][image4]  
3Blue1Brown. But what is a GPT? Visual intro to transformers | Chapter 5, Deep Learning. [https://youtu.be/wjZofJX0v4M](https://youtu.be/wjZofJX0v4M?si=sVIkaUQBmG3fVuiB&t=915)

Die Bedeutungen eines Wortes werden in einem groÃŸen Sprachmodell durch dessen Position im Vektorraum dargestellt. Viele Richtungen in diesem Vektorraum korrelieren dabei metaphorisch mit bestimmten Bedeutungsachsen oder Eigenschaften â€“ wie etwa â€œMusik vs. Politikâ€, â€œweiblich vs. mÃ¤nnlichâ€ oder â€œabstrakt vs. konkretâ€. Isoliert sind einzelne Dimensionen jedoch kaum nutzbar. Ein Wort startet zwar immer mit einem festgelegten Initial-Embedding, doch sobald es in einem konkreten Satz verwendet wird, berechnet das Modell schichtweise neue, kontextabhÃ¤ngige Vektoren entlang dieser Achsen. Dadurch landet etwa das Wort â€œQueenâ€ in einem Satz Ã¼ber britische Monarchie nahe bei Konzepten wie â€œKroneâ€ und â€œPalastâ€, in einem musikalischen Kontext nahe â€œRockâ€ und â€œFreddie Mercuryâ€, und im Zusammenhang mit Drag nahe Begriffen wie â€œPerformanceâ€ oder â€œMake-upâ€. Die Vielzahl solcher Bedeutungsdimensionen macht es mÃ¶glich, dass Sprachmodelle Nuancen erfassen und die Bedeutung eines Wortes je nach Kontext flexibel abbilden kÃ¶nnen. Oder auch daran scheitern. 

Diese Strukturen entstehen wÃ¤hrend des Trainings direkt aus den Daten und sind fÃ¼r Menschen nicht immer logisch nachvollziehbar. Unterschiedliche Sprach- und Schreibstile aktivieren implizit unterschiedliche Regionen im Embedding-Raum. So aktiviert â€œ*The King doth wake tonight and takes his rouse*â€ stilistische Merkmale von Shakespeare, wÃ¤hrend â€œ*The King wakes up tonight and begins his celebration*â€ moderne SprachrÃ¤ume nutzt. Das hat einen Einfluss auf das Prompt Engineering. Ein prÃ¤zise wissenschaftlicher Prompt navigiert das Modell anders als ein kreativ formulierter Prompt. Die gezielte Modellierung von Experten-Personas (â€œ*You are an expert inâ€¦*â€) ist effektiv, da sie spezifische Verarbeitungswege im Modell aktiviert. Diese Wege wirken wie metaphorische Programme, die statistisch gelernt wurden, obwohl sie eigentlich keine expliziten Programme sind. Ich betrachte sie als Programme, die ich per Prompt ein-, aus- oder kombinieren kann.

## **Context Window**

Das *Context Window*[^16] legt fest, wie viele Token ein Modell gleichzeitig â€œim Blickâ€ behalten kann. Es funktioniert Ã¤hnlich wie ein temporÃ¤res ArbeitsgedÃ¤chtnis, das sich allerdings an nichts erinnern kann. WÃ¤hrend frÃ¼he Modelle wie GPT-2 auf 1.024 Token limitiert waren, kÃ¶nnen moderne Systeme wie Claude 3.5 bis zu 200.000 Token verarbeiten. Googles Gemini kann sogar eine bis zwei Million multimodale Token verarbeiten, darunter auch Bild-, Audio- und Videodaten. 

Wird das Context Window Ã¼berschritten, verliert das Modell den Zugriff auf frÃ¼here Informationen. Es muss dann aufgrund des verbleibenden Kontexts Vermutungen anstellen, was zu vermehrten Halluzinationen fÃ¼hren kann. Neben Input und Output kann das Context Window auch versteckte Systemanweisungen, externe Dokumente, Code-Schnipsel und Ergebnisse aus Retrieval-Mechanismen (RAG) enthalten, was die verfÃ¼gbare KapazitÃ¤t zusÃ¤tzlich belastet. Alles auÃŸerhalb des Fensters ist fÃ¼r das Modell nicht mehr erreichbar, sondern hÃ¶chstens indirekt Ã¼ber bereits gelerntes Wissen verfÃ¼gbar. Um erneut darauf zugreifen zu kÃ¶nnen, ist ein externer Retrieval- oder Memory-Mechanismus nÃ¶tig, der die entsprechenden Token wieder in das Context Window lÃ¤dt. Dies kann ein LLM wiederum Ã¼ber Tool Use realisieren.

Ein grÃ¶ÃŸeres Context Window fÃ¼hrt bei klassischen Transformers jedoch zu einem hÃ¶heren Rechenaufwand, da die Last quadratisch mit der Tokenzahl steigt. Es ist also nicht nur teurer, sondern es kommt auch zu einem Leistungsabfall, wenn sie relevante Informationen aus der Mitte eines langen Kontextes berÃ¼cksichtigen mÃ¼ssen. Zudem werden sie anfÃ¤lliger fÃ¼r versteckte schÃ¤dliche Eingaben (*Jailbreaking*[^17]). Es zeigt sich jedoch auch, dass Context Windows theoretisch unendlich groÃŸ werden kÃ¶nnen.[^18]

![][image5]

Die Abbildung zeigt die Grenze des Context Windows. Links passen 6 000 **Input-Token** plus 1 500 **Output-Token** problemlos in das 8K-Fenster. Es gibt also separate Obergrenzen fÃ¼r Input und Output. Typischerweise wird ein Teil des Fensters fÃ¼r die Antwort reserviert, sodass die maximal mÃ¶gliche Output-LÃ¤nge kleiner ist als die erlaubte EingabegrÃ¶ÃŸe â€“ das ist aber eine Designentscheidung, kein Naturgesetz des Modells. Rechts hingegen wird mit 10 000 Input-Token plus 1 500 Output-Token die maximale Tokenanzahl Ã¼berschritten. Die rot markierten 3 500 Token am Anfang des Inputs stehen wÃ¤hrend der Antwortgenerierung nicht mehr explizit zur VerfÃ¼gung.[^19] Kopiert man ein Buch hinein, und selbst wenn ganz am Anfang die Erde explodiert, verarbeitet das Modell diese Information nicht mehr aktiv, sobald das Context Window ausreichend gefÃ¼llt wurde. Allerdings kann das Modell dennoch scheinbar â€œwissenâ€, dass die Erde explodierte, entweder weil diese Information Teil seines allgemeinen â€œWeltwissensâ€ ist (z. B. bei bekannten Werken), oder weil es eine plausible Fortsetzung aus dem aktuellen Kontext extrapoliert (â€œhalluziniertâ€). Dies unterscheidet sich fundamental davon, wenn die ursprÃ¼nglichen Token tatsÃ¤chlich noch aktiv im Context Window vorhanden wÃ¤ren.

## **Training**

â€œWir zÃ¼chten diese Modelle mehr, als dass wir sie bauenâ€, formuliert es Dario Amodei.[^20] Selbst die fÃ¼hrenden LLM-Erzeuger wissen nicht exakt, wie ihre Modelle â€œtatsÃ¤chlichâ€ funktionieren oder welche exakte Konfiguration optimal ist. Vielmehr verstehen sie, welche Parameter in unterschiedlichen Trainingsphasen gezielt angepasst werden mÃ¼ssen.

Das Training von LLMs erfolgt typischerweise in drei Phasen, die das System jeweils auf spezifische Weise prÃ¤gen. In der ersten, sehr ressourcenintensiven Phase, dem **Pre-Training**, lernen LLMs mittels selbstÃ¼berwachtem Lernen anhand von Billionen von Token aus verschiedensten Quellen. Ein Prozess, den Andrej Karpathy[^21] als Kompression des â€œInternets als ZIP-Dateiâ€[^22] bezeichnet. Autoregressive Architekturen[^23] wie GPT optimieren dabei die Vorhersage des nÃ¤chsten Tokens (**Next-Token-Prediction**). Durch das Verarbeiten von Webseiten, wissenschaftlichen Publikationen, Code-Repositories und BÃ¼chern entstehen in den Milliarden von Parametern statistische ReprÃ¤sentationen der Welt.[^24] Was ich zuvor salopp als â€œWeltwissenâ€ bezeichnete, ist im Sinne meiner System-1.42-Metapher eher eine verzerrte, mit Bias beladene Abstraktion von â€œWeltwissenâ€. Wie sich Gravitation tatsÃ¤chlich anfÃ¼hlt, lÃ¤sst sich durch keinen Text oder Video erlernen. Das Modell durchsucht bei einer Anfrage nicht das Web, sondern extrahiert Wahrscheinlichkeitsverteilungen. Es reprÃ¤sentiert, wie Karpathy sagt, die â€œGestaltâ€ gelernter Texte. Dabei erfasst das Modell grobe syntaktische und semantische Muster, erwirbt aber noch keinerlei Spezialisierung.

Erst in der zweiten Phase, dem **Supervised Instruction Fine-Tuning**, erhÃ¤lt das Modell eine spezialisierte Aufgabe und konkrete Anweisungen dazu. Kuratierte Instruktion-Antwort-Paare verwandeln das rohe Basis-Modell in spezifische Anwendungsformen wie Chat- oder Coding-Modelle. In dieser Phase lernt das Modell, seine latenten statistischen ReprÃ¤sentationen in konkrete, aufgabenorientierte und kohÃ¤rente Outputs umzusetzen. Hier wird festgelegt, wie die Antworten aussehen sollen: kurz oder ausfÃ¼hrlich, im JSON-Format oder in Form eines typischen Frage-Antwort-Spiels eines Chatbots.

In der dritten Phase, dem **Reinforcement Learning[^25] from Human Feedback**[^26], optimiert das Modell sein Verhalten anhand menschlicher PrÃ¤ferenzen. Dabei geht es nicht primÃ¤r um faktenbasierte Korrektheit â€“ obwohl diese durchaus gelegentlich verbessert wird â€“ sondern vor allem um weichere, komplexere QualitÃ¤tsdimensionen wie Hilfsbereitschaft, Sicherheit oder Stil. Viele dieser Kriterien lassen sich nicht einfach fest definieren oder eindeutig labeln. Stattdessen bewerten Menschen Antworten vergleichend (â€œAntwort A ist besser als Antwort Bâ€œ). So lernt das Modell, weiche Ziele wie â€œnÃ¼tzlich und sicherâ€ anzustreben, ohne dass jede Eigenschaft separat und explizit definiert werden muss. Diese Phase birgt jedoch auch das Risiko, dass menschlicher Bias zusÃ¤tzlich zum Bias in den Trainingsdaten direkt in das Modell Ã¼bertragen wird.

Token als atomare Einheiten fÃ¼r die Syntax, Embeddings als semantische ReprÃ¤sentationen und das Context Window als unmittelbarer Kontext bilden gemeinsam die technische Grundlage eines LLMs. Auf diesen Komponenten basiert unmittelbar die Transformer-Architektur mit ihrer Kernfunktion, der Next-Token-Prediction.

# **Transformer und Next-Token-Prediction**

LLMs basieren auf der sogenannten ***Transformer-Architektur***[^27]. Im Gegensatz zu Ã¤lteren Modellen analysieren Transformer Texte nicht sequenziell Wort fÃ¼r Wort, sondern verarbeiten die Token innerhalb jeder Verarbeitungsschicht parallel. Dies gelingt durch den ***Attention-Mechanismus***[^28], der berechnet, wie relevant jedes Token fÃ¼r die Bedeutung anderer Token im Text ist. Ein entscheidender Vorteil ist, dass Transformer auch AbhÃ¤ngigkeiten Ã¼ber groÃŸe Distanzen hinweg erfassen kÃ¶nnen, etwa wenn sich eine Instruktion am Anfang des Prompts auf Inhalte bezieht, die erst Tausende Token spÃ¤ter folgen. Der Attention-Mechanismus operiert innerhalb eines festgelegten Context Windows. Das Context Window begrenzt also die Zahl der sichtbaren Token, wÃ¤hrend Attention deren wechselseitige Relevanz berechnet.

Diese Architektur ermÃ¶glicht es LLMs, das nÃ¤chste Token vorherzusagen, ein Prozess, der als Next-Token-Prediction bezeichnet wird. Bei der Eingabe â€œThe cat sat on theâ€¦â€ berechnet das Modell aus den Attention-Mustern, dass â€œmatâ€ mit hoher Wahrscheinlichkeit folgen sollte. Nicht, weil es weiÃŸ, dass Katzen auf Matten sitzen, sondern weil diese Wortfolge statistisch hÃ¤ufig auftritt. Der Attention-Mechanismus erlaubt es, komplexe Beziehungen zwischen entfernten WÃ¶rtern zu erfassen. Beispielsweise kann er erkennen, dass sich â€œitâ€ auf das zuvor erwÃ¤hnte â€œanimalâ€ bezieht. Aber wenn man alle mÃ¶glichen Texte kennen wÃ¼rde, kÃ¶nnte man sich dann dem â€œrichtigenâ€ Kontext nicht ziemlich gut annÃ¤hern?

![][image6]  
*Next-Token-Prediction und Attention-Mechanismus. Links: Das neuronale Netz sagt basierend auf vier EingabewÃ¶rtern das nÃ¤chste Wort vorher. Rechts: Der Attention-Mechanismus zeigt, wie das Modell Beziehungen zwischen WÃ¶rtern herstellt. [https://jalammar.github.io/illustrated-transformer](https://jalammar.github.io/illustrated-transformer)* 

Da Modelle Token stets sequentiell erzeugen und gewÃ¤hlte Token nicht nachtrÃ¤glich Ã¤ndern kÃ¶nnen, entsteht ein Effekt Ã¤hnlich einem **â€œSchmetterlingseffektâ€**: Ein einzelnes Token zu Beginn beeinflusst den gesamten Kontext aller nachfolgenden Vorhersagen. Beispielsweise aktiviert â€œdogâ€ als erstes Token andere statistische Muster als â€œcatâ€. Dadurch verÃ¤ndert sich die Wahrscheinlichkeitsverteilung nachfolgender Token erheblich. Hundethemen wie â€œBellenâ€ oder â€œSpaziergÃ¤ngeâ€ werden wahrscheinlicher, wÃ¤hrend Katzenthemen wie â€œKratzbÃ¤umeâ€ und â€œSchnurrenâ€ unwahrscheinlicher werden.[^29]

Die Auswahl des nÃ¤chsten Tokens wird durch drei Hauptparameter gesteuert: Die ***Temperature*** reguliert die ZufÃ¤lligkeit der Ausgabe und reicht von eher deterministisch bei 0 bis â€œhochkreativâ€ bei 2\. Alles ab 1 ist jedoch extrem â€œkreativâ€. Eine hÃ¶here Temperature ermÃ¶glicht die Exploration eines breiteren Bereichs mÃ¶glicher LÃ¶sungswege. Paradoxerweise kÃ¶nnen dadurch bei komplexen Problemen bessere Ergebnisse erzielt werden, wenn auch weniger reproduzierbare, da das Modell aus lokalen Optima[^30] befreit und unkonventionelle AnsÃ¤tze erkundet werden. **Top-K** begrenzt die Auswahl auf die K wahrscheinlichsten Token und **Top-P** nur Token berÃ¼cksichtigt, deren kumulative Wahrscheinlichkeit einen Schwellenwert P nicht Ã¼berschreitet. FÃ¼r die meisten Nutzer:innen genÃ¼gt es, die Temperature einzustellen: niedrige Werte (0â€“0,4) fÃ¼r stabile, vorhersehbare Ergebnisse, hÃ¶here Werte (0,4â€“1) fÃ¼r kreativere, vielfÃ¤ltigere Ausgaben.[^31]

# **Autocomplete?**

Was bedeutet es konkret, einen Kontext mit allen Details und AbhÃ¤ngigkeiten korrekt zu erfassen? Ilya Sutskever, MitgrÃ¼nder von OpenAI, argumentiert, dass *Next-Token-Prediction* deutlich mehr umfasst als bloÃŸes *Autocomplete*. Um zuverlÃ¤ssig das richtige nÃ¤chste Token vorherzusagen, mÃ¼sse ein System ein implizites â€œVerstehen der zugrunde liegenden RealitÃ¤tâ€ entwickeln. Ein Sprachmodell konstruiert daher aus Kontextinformationen eine probabilistische ReprÃ¤sentation mÃ¶glicher RealitÃ¤ten, indem es gelernte Muster und statistische ZusammenhÃ¤nge rekonstruiert. Ein ausreichend intelligentes System kÃ¶nnte, so Sutskever weiter, sogar kreativ extrapolieren und sich dabei vorstellen, wie eine hypothetische Person mit â€œgroÃŸer Einsicht und Weisheitâ€ in einer gegebenen Situation handeln wÃ¼rde[^32], eine Vision, die Andrej Karpathys Charakterisierung von LLMs als â€œpeople spiritsâ€[^33], also stochastischen Simulationen menschlichen Verhaltens, sehr Ã¤hnlich ist. Das ist natÃ¼rlich eine metaphorische und stark vereinfachte, beinah spirituell anmutende Darstellung. Als Denkkonzept hilft sie mÃ¶glicherweise jedoch beim praktischen Umgang mit LLMs, selbst wenn sie stark vermenschlichend wirkt. Amanda Askell von Anthropic warnt Ã¼brigens vor zwei Extremen im Umgang mit LLMs: Sie entweder zu sehr oder zu wenig zu vermenschlichen. Der optimale Mittelweg, bei dem die Systeme durch menschliche Kommunikationsmuster und Human Feedback trainiert wurden, fÃ¼hrt zu besseren Ergebnissen.[^34]

Zwar bilden Sprachmodelle durchaus breit angelegte konzeptuelle Kategorien, die jenen Ã¤hnlich sind, die Menschen intuitiv nutzen. Doch bei feineren semantischen Unterscheidungen bleibt ihre interne ReprÃ¤sentation oft oberflÃ¤chlich. Im Gegensatz zur menschlichen Wahrnehmung speichern Sprachmodelle Muster in stark komprimierter Form. Dabei gehen feine Unterschiede und adaptive Details verloren, wodurch das von Modellen aufgebaute Bild der Welt zwar plausibel, jedoch vergleichsweise grob bleibt.[^35] Es gibt allerdings konkrete Hinweise darauf, dass Modelle intern komplexe, mehrstufige Prozesse vollziehen. Beispielsweise aktivieren Sprachmodelle zunÃ¤chst Zwischenschritte oder ZwischenreprÃ¤sentationen, bevor sie zu einer endgÃ¼ltigen Ausgabe gelangen. Soll etwa die Hauptstadt eines Bundesstaates genannt werden, in dem sich eine bestimmte Stadt befindet (wie Dallas), aktiviert Claude 3.5 intern zunÃ¤chst eine Zwischendarstellung (â€œTexasâ€) und liefert dann die passende Antwort (â€œAustinâ€). Ã„hnliches gilt fÃ¼r Aufgaben wie das Erzeugen von Reimen. Modelle berÃ¼cksichtigen intern frÃ¼hzeitig mÃ¶gliche ReimwÃ¶rter, die den Satzbau wesentlich beeinflussen.[^36]

# **Emergenz? Grokking?**

FÃ¼hrt das Ermitteln des nÃ¤chsten Token in einem ausreichend groÃŸen und im Post-Training adaptierten neuronalen Netz tatsÃ¤chlich zu Zwischenschritten und Vorausplanen? LÃ¤sst sich das noch als bloÃŸes Autocomplete erklÃ¤ren? TatsÃ¤chlich konnten groÃŸe Sprachmodelle ab einer kritischen GrÃ¶ÃŸe plÃ¶tzlich komplexe logische und arithmetische Aufgaben lÃ¶sen, die kleinere Modelle nicht bewÃ¤ltigten.[^37] Solche FÃ¤higkeitssprÃ¼nge bezeichnet man als **emergente Eigenschaften**, definiert als FÃ¤higkeiten, die bei kleinen Modellen nicht vorhanden sind und erst ab einer bestimmten Parameterzahl sichtbar werden, ohne dass dies durch lineare Skalierung vorhersehbar ist.[^38] 

Bei groÃŸen Modellen scheinen FÃ¤higkeiten oft abrupt aufzutreten, was jedoch meist auf binÃ¤re Messmethoden ("kann/kann nichtâ€œ) zurÃ¼ckzufÃ¼hren ist, wÃ¤hrend die tatsÃ¤chliche Entwicklung kontinuierlich verlÃ¤uft.[^39] Dennoch existieren empirisch belegte FÃ¤lle, in denen Modelle komplexe interne AblÃ¤ufe entwickeln, die Ã¼ber reines Autocomplete hinausgehen. Diese Fortschritte sind allerdings stark abhÃ¤ngig von Trainingsbedingungen.[^40] Daher bleibt offen, ob es sich um echte Emergenz handelt oder lediglich um eine Ã¼berzeugende Simulation dank umfangreicherer Trainingsdaten. Denn Modelle simulieren nicht nur reines â€œWeltwissenâ€, sondern entwickeln zunehmend unterschiedliche interne â€œProgrammeâ€, also spezifische ProblemlÃ¶sungsstrategien. Eines dieser ProblemlÃ¶sungsprogramme ist Ã¼brigens â€œLetâ€™s think step by stepâ€. Dieses sogenannte â€œChain-of-Thoughtâ€ (CoT) wurde erst nachtrÃ¤glich entdeckt und wird hÃ¤ufig als emergente Eigenschaft interpretiert, ist allerdings weiterhin Gegenstand aktueller Debatten.  

Ein neuronales Netz zeigt beim sogenannten **Grokking**[^41] nach abgeschlossenem Training plÃ¶tzlich einen Ãœbergang von reiner Memorierung zu echter Generalisierung. Dieses PhÃ¤nomen trat bislang hauptsÃ¤chlich bei speziellen, kÃ¼nstlich erzeugten Aufgaben auf, bei denen kleine DatensÃ¤tze, genau eingestellte Trainingsparameter sowie eine starke Regulierung entscheidend waren. Grokking wird jedoch zunehmend als mÃ¶gliches Trainingsartefakt interpretiert. Unklar bleibt weiterhin, ob neuronale Netze tatsÃ¤chlich abstrakte Regeln generalisieren oder nur Ã¼berzeugend simulieren. Diese Unsicherheit zeigt sich besonders deutlich bei der umstrittenen FÃ¤higkeit des sogenannten "Reasoningsâ€œ, also logischer Schlussfolgerungen, die typischerweise bei sehr groÃŸen Modellen untersucht wird. Dabei bleibt offen, ob Reasoning eine echte emergente Eigenschaft skalierender Modelle darstellt oder vielleicht eher ein Grokking-Ã¤hnliches PhÃ¤nomen ist, also ein plÃ¶tzlicher Durchbruch nach bereits abgeschlossenem Training.

# **Reasoning?**

LLMs rufen bei Reasoning-Aufgaben selten direkte Antworten aus Faktenwissen ab, sondern greifen primÃ¤r auf prozedurales Wissen zurÃ¼ck â€“ etwa auf gespeicherte LÃ¶sungsstrategien, Codes, Formeln oder Schritt-fÃ¼r-Schritt-Anleitungen.[^42] Doch reicht es aus, lediglich das prozedurale Schema einer ProblemlÃ¶sung zu kennen, um von "echtemâ€œ Reasoning zu sprechen? FranÃ§ois Chollet, der Intelligenz als â€œdie Effizienz eines Systems im Erwerb neuer FÃ¤higkeiten zur BewÃ¤ltigung spezifischer Situationenâ€ definiert, unterscheidet Reasoning klar davon. Er versteht darunter explizit â€œdie FÃ¤higkeit, neue Programme (aus vorhandenen Programmteilen) fÃ¼r bisher ungesehene Aufgaben zu synthetisierenâ€. Intelligentes Verhalten verlangt demnach mehr als bloÃŸes Schemawissen: Es erfordert, erworbene FÃ¤higkeiten auf neuartige Weise zu kombinieren und dadurch Probleme zu lÃ¶sen, die zuvor unbekannt waren.

Das Paper *The Illusion of Thinking* zeigt, dass Large Reasoning Models mit steigender KomplexitÃ¤t der Aufgaben zunÃ¤chst scheinbar besser, anschlieÃŸend jedoch zunehmend schlechter performen und schlieÃŸlich vollstÃ¤ndig scheitern.[^43] Dies scheint Chollets[^44] und Kambhampatis[^45] Argument zu unterstÃ¼tzen, wonach Modelle primÃ¤r auf memorierte Prozeduren zurÃ¼ckgreifen und nicht genuin logisch schlussfolgern kÃ¶nnen. Allerdings untersucht diese Studie nur isolierte Modelle, ohne Tool Use und ohne zusÃ¤tzliche Test-Time Compute, zwei Kernelemente moderner Frontier-LLMs. Sie bietet daher keine umfassende Bewertung des aktuellen Standes. Zum Vergleich: OpenAIs o3-Modell erzielte auf Chollets ARC-AGI[^46] Benchmark, der speziell fÃ¼r neuartige, abstrakte Reasoning-Aufgaben entwickelt wurde, bemerkenswerte 87,5 %. Dieses Ergebnis veranlasste Chollet sogar dazu, seine ursprÃ¼ngliche EinschÃ¤tzung der Modelle von â€œnicht intelligentâ€ zu â€œnicht nicht-intelligentâ€ zu revidieren.

Chollet insistiert auf einer anspruchsvollen Definition von Reasoning als "On-the-fly-Synthese neuer Programme aus vorhandenen Programmteilen fÃ¼r bisher ungesehene Aufgabenâ€. LLMs operieren jedoch fundamental anders: Sie generieren sequenziell Token, um Kontexte schrittweise anzureichern und daraus statistisch wahrscheinlichere Folgetoken abzuleiten. Chollet sieht LLMs nicht als diskrete Programmgeneratoren im klassischen Sinn, doch er bezeichnet ihre brute-force Suche Ã¼ber Chainâ€‘ofâ€‘Thought-Pfade durchaus als eine Form von Programmsynthese. In seinen Worten: â€œProgram synthesis is a treeâ€‘search process. Use LLMs to guide this treeâ€‘search process.â€ Diese tokenbasierte Strategie stÃ¶ÃŸt allerdings bei komplexen, kompositorischen Aufgaben an Grenzen, wie unter anderem das Apple-Paper oder ARC-AGI-2 zeigen.[^47] Chollet selbst charakterisiert o3[^48] als â€œnatÃ¼rlichsprachlichen Suchprozess, der den kompletten Raum mÃ¶glicher Chain-of-Thought LÃ¶sungen durchsuchtâ€.

Noam Brown[^49], einer der Hauptentwickler, demonstrierte bereits in frÃ¼heren Arbeiten, dass zusÃ¤tzliche Rechenzeit bei der Inferenz, also wÃ¤hrend der Tokengenerierung, einem Skalierungseffekt von bis zu 100.000x entspricht. Der entscheidende Durchbruch bei OpenAIs Modell o1 basiert auf der Erkenntnis des **"Verifier-Generator Gap"**: Bei vielen Reasoning-Aufgaben ist es einfacher, LÃ¶sungen zu verifizieren als sie zu generieren, Ã¤hnlich wie beim Sudoku, wo die ÃœberprÃ¼fung deutlich leichter ist als die eigentliche LÃ¶sungsfindung. Modelle wie o1 und o3 nutzen dies gezielt aus und generieren durch ***Reinforcement Learning*** zahlreiche "Denkschritte" in Form von "Thinking Token", erkennen Fehler und optimieren LÃ¶sungswege. Was wie ein chaotischer Prozess wirkt, ist eine Form der Exploration des LÃ¶sungsraums, die durch die inhÃ¤rente VerifikationsfÃ¤higkeit von LLMs gesteuert wird.[^50] Diese strukturierte Exploration erlaubt es ihnen, tatsÃ¤chlich Ã¼ber die reine Interpolation von Trainingsdaten hinauszugehen und echte Extrapolation zu betreiben, indem sie unbekannte Bereiche des LÃ¶sungsraums navigieren ("uncharted areas").

Die massive Skalierung von Test Time Compute verstÃ¤rkt diese FÃ¤higkeiten: Mehr Rechenzeit erlaubt sowohl breitere Exploration (mehr generierte Pfade) als auch prÃ¤zisere Selektion (bessere Verifikation). Ob dies zu "echtem" Reasoning fÃ¼hrt, bleibt offen â€“ aber die funktionale Ã„quivalenz wird zunehmend ununterscheidbar. LLMs bleiben Next-Token-Predictors, doch durch die Orchestrierung von Generierung und Verifikation bei massiver Skalierung entsteht ein System, das erfolgreich Reasoning-Aufgaben lÃ¶st, auch wenn der zugrundeliegende Mechanismus fundamental anders ist als menschliches Denken.

Die FÃ¤higkeit von LLMs, sogenannte **Halluzinationen**[^51] zu erzeugen, also Informationen zu generieren, die Ã¼ber das explizit Gelernt hinausgehen, ist kein Fehler, sondern eine fundamentale Eigenschaft ihrer Architektur. Halluzinationen sind problematisch, wenn sie falsche oder irrefÃ¼hrende Inhalte erzeugen. Sie kÃ¶nnen jedoch auch wertvoll sein, wenn sie kreative Ideen oder neuartige LÃ¶sungsansÃ¤tze hervorbringen, die nicht direkt aus den Trainingsdaten ableitbar sind. LLMs kÃ¶nnen Halluzinationen nicht vollstÃ¤ndig vermeiden, da sie diese als explorativen Mechanismus benÃ¶tigen, um komplexe Aufgaben zu lÃ¶sen. Deshalb sind sorgfÃ¤ltig konzipierte Workflows notwendig, um Ergebnisse systematisch zu verifizieren, den Kontext aktiv zu managen und Halluzinationen gezielt zu steuern. Dabei mÃ¼ssen wir uns bewusst sein, dass es sich um halluzinierende â€œSystem-1.42-Systemeâ€ handelt und nicht um menschliche Intelligenz.

Verstehen und Denken sind Eigenschaften, die wir Menschen vorbehalten wollen und sollen. Doch LLMs stellen diese Grenzen infrage, indem sie Verhalten simulieren, das Denken Ã¤hnelt, ohne dabei echtes VerstÃ¤ndnis oder zuverlÃ¤ssige SelbstprÃ¼fung zu besitzen. Mit dem Begriff â€œSystem 1.42â€ mÃ¶chte ich diese Ambivalenz hervorheben. Die entscheidende Parallele zu Douglas Adamsâ€™ *Per Anhalter durch die Galaxis* ist dabei weniger die berÃ¼hmte Antwort â€œ42â€, sondern vielmehr die Reaktion der â€œGewerkschaft der Philosophen und Denkerâ€, die den Supercomputer Deep Thought abschalten mÃ¶chte, weil er ihre Existenzberechtigung bedroht. Wenn LLMs funktional nicht mehr von menschlichem Reasoning, Coding oder Forschen unterscheidbar sind, dann sind ihre Auswirkungen dieselben. Die Zukunft liegt deshalb nicht darin, LLMs zu hypen, zu mystifizieren oder zu verteufeln, sondern darin, sie kritisch und gezielt als halluzinierende Reasoner einzusetzen. Verschwinden werden sie nicht mehr, hÃ¶chstens durch etwas Besseres ersetzt. Ihr Einfluss auf die Wissensarbeit â€“ und weniger auf Routineprozesse â€“ wird aus meiner Sicht bislang noch nicht ausreichend diskutiert, obwohl mit Stand Juli 2025 eine weitere Skalierung und Verbesserung sehr wahrscheinlich ist. Wie die System-1.42-Modelle selbst befinde ich mich daher in einem Zwischenzustand zwischen â€œDon't Panic\!â€ und â€œPanic\!â€. Diese Ambivalenz ist vielleicht die einzig ehrliche Haltung gegenÃ¼ber einer Technologie, deren Evolution schneller verlÃ¤uft, als unser Verstehen folgen kann.

[^1]:  Die Autor:innen argumentieren, dass LLMs lediglich linguistische Form manipulieren, ohne Zugang zu Bedeutung zu haben, und warnen vor den Risiken immer grÃ¶ÃŸerer Modelle. Bender, Emily M., Timnit Gebru, Angelina McMillan-Major, und Shmargaret Shmitchell. "On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? ğŸ¦œâ€œ. In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, 610â€“23. FAccT â€™21. New York, NY, USA: Association for Computing Machinery, 2021\. [https://doi.org/10.1145/3442188.3445922](https://doi.org/10.1145/3442188.3445922). 

[^2]:  Ãœberblicksarbeit, die zeigt, wie aktuelle GroÃŸmodelle dank Such-, Belohnungs- und SelbstverbesserungsÂ­verfahren vom reinen Musterabgleich zu â€œReasoningâ€ gelangen â€“ und zugleich erklÃ¤rt, wo diese neuen â€œReasoningâ€-FÃ¤higkeiten ihre Grenzen haben. Li, Zhong-Zhi, Duzhen Zhang, Ming-Liang Zhang, Jiaxin Zhang, Zengyan Liu, Yuxuan Yao, Haotian Xu, et al. â€˜From System 1 to System 2: A Survey of Reasoning Large Language Modelsâ€™. arXiv, 5 June 2025\. [https://doi.org/10.48550/arXiv.2502.17419](https://doi.org/10.48550/arXiv.2502.17419). 

[^3]:  Die Skalierungsthesen beschreiben den systematischen, mathematisch vorhersehbaren Zusammenhang zwischen der LeistungsfÃ¤higkeit von Sprachmodellen und den drei zentralen Faktoren: ModellgrÃ¶ÃŸe (Anzahl der Parameter), Umfang der Trainingsdaten und eingesetzte Rechenleistung. Sie besagen, dass die Modellleistung bei gezielter VergrÃ¶ÃŸerung dieser Faktoren nicht zufÃ¤llig, sondern durch einfache mathematische Potenzgesetze verbessert wird; somit erlauben Skalierungsthesen prÃ¤zise Prognosen Ã¼ber optimale ModellgrÃ¶ÃŸe, Datenmenge und Rechenzeit, um maximale Leistung effizient zu erreichen. Kaplan, Jared, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. â€˜Scaling Laws for Neural Language Modelsâ€™. arXiv, 23 January 2020\. [https://doi.org/10.48550/arXiv.2001.08361](https://doi.org/10.48550/arXiv.2001.08361).

[^4]:  Shojaee, Parshin, Iman Mirzadeh, Keivan Alizadeh, Maxwell Horton, Samy Bengio, und Mehrdad Farajtabar. â€œThe Illusion of Thinking: Understanding the Strengths and Limitations of Reasoning Models via the Lens of Problem Complexityâ€, 2025\. [https://ml-site.cdn-apple.com/papers/the-illusion-of-thinking.pdf](https://ml-site.cdn-apple.com/papers/the-illusion-of-thinking.pdf)

[^5]:  Zur Veranschaulichung grundlegender Unterschiede zwischen menschlicher Kognition und den probabilistischen Mechanismen von LLMs, insbesondere hinsichtlich Lernen, Verarbeitung, GedÃ¤chtnis und vermeintlichem Reasoning. IBM Technology. AI vs Human Thinking: How Large Language Models Really Work. [https://youtu.be/-ovM0daP6bw](https://youtu.be/-ovM0daP6bw)

[^6]:  Beispielsweise scheitern LLMs an einfachen rÃ¤umlichen und zeitlichen Aufgaben.

[^7]:  Agentic Systems sind allgemein gesagt KI-Systeme, die modular aufgebaute Komponenten kombinieren, typischerweise einschlieÃŸlich groÃŸer KI-Modelle, um Aufgaben durch Planung, Entscheidungsfindung, Werkzeugnutzung und iterative Selbstreflexion zu lÃ¶sen. Hu, Shengran, Cong Lu, and Jeff Clune. â€˜Automated Design of Agentic Systemsâ€™. arXiv, 2 March 2025\. [https://doi.org/10.48550/arXiv.2408.08435](https://doi.org/10.48550/arXiv.2408.08435).

[^8]:  Tool Use bezeichnet die FÃ¤higkeit von LLMs, externe Funktionen und APIs wÃ¤hrend der Textgenerierung aufzurufen und deren Ergebnisse zu integrieren.

[^9]:  Kahneman, Daniel. Thinking, fast and slow. New York, NY, US: Farrar, Straus and Giroux, 2011\.

[^10]:  Lightman, Hunter, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. â€˜Letâ€™s Verify Step by Stepâ€™. arXiv, 31 May 2023\. [https://doi.org/10.48550/arXiv.2305.20050](https://doi.org/10.48550/arXiv.2305.20050). 

[^11]:  Oxford Professor: AIs are strange new minds. [https://youtu.be/35r0iSajXjA](https://youtu.be/35r0iSajXjA)

[^12]:  Yann LeCun argumentiert, dass autoregressive LLMs â€œfundamental fehlerhaftâ€ seien und â€œin ein paar Jahren niemand mehr verwendet werden wirdâ€. Als Alternative schlÃ¤gt er Energy-based Models mit "Inference by Optimization" vor, die echte Validierung durch iterative Optimierung ermÃ¶glichen wÃ¼rden. LeCun betont, dass solche Systeme â€œSystem 2â€-Ã¤hnliche Aufgaben durch mehrstufige Optimierungsprozesse lÃ¶sen kÃ¶nnten \- im Gegensatz zu LLMs, die nur durch â€œChain of Thoughtsâ€ (mehr Token generieren) komplexere Probleme angehen kÃ¶nnen. Yann LeCun â€œMathematical Obstacles on the Way to Human-Level AIâ€. [https://youtu.be/ETZfkkv6V7Y](https://youtu.be/ETZfkkv6V7Y?si=WLOGyXNViqql6wP7)

[^13]:  Andrej Karpathy. Let's build the GPT Tokenizer. [https://youtu.be/zduSFxRajkE](https://youtu.be/zduSFxRajkE?si=kFYNuF1fXCPUpOs6)

[^14]:  Es gibt sprach- oder domÃ¤nenspezifische Tokenizer.

[^15]:  Bei Frontier-Modellen mit Websuche dÃ¼rfte das aber deutlich besser funktionieren. Berglund, Lukas, Meg Tong, Max Kaufmann, Mikita Balesni, Asa Cooper Stickland, Tomasz Korbak, und Owain Evans. "The Reversal Curse: LLMs trained on â€šA is Bâ€˜ fail to learn â€šB is Aâ€˜â€œ. arXiv, 26\. Mai 2024\. [https://doi.org/10.48550/arXiv.2309.12288](https://doi.org/10.48550/arXiv.2309.12288). 

[^16]:  What is a Context Window? Unlocking LLM Secrets. [https://youtu.be/-QVoIxEpFkM](https://youtu.be/-QVoIxEpFkM)

[^17]:  ChatGPT Jailbreak \- Computerphile. [https://youtu.be/zn2ukSnDqSg](https://youtu.be/zn2ukSnDqSg)

[^18]:  Munkhdalai, Tsendsuren, Manaal Faruqui, und Siddharth Gopal. 2024\. Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention. arXiv. [https://doi.org/10.48550/arXiv.2404.07143](https://doi.org/10.48550/arXiv.2404.07143)

[^19]:  Es gibt allerdings Mechanismen, um diese Limitierung zu umgehen. Prompt Caching beispielsweise ist eine API-Funktion, bei der hÃ¤ufig verwendete Prompt-Teile (wie Systemanweisungen oder Dokumente) serverseitig zwischengespeichert werden, wodurch sie nicht bei jeder Anfrage erneut das Context Window belegen. [https://www.anthropic.com/news/prompt-caching](https://www.anthropic.com/news/prompt-caching)

[^20]:  The Future of U.S. AI Leadership with CEO of Anthropic Dario Amodei. [https://www.youtube.com/watch?v=esCSpbDPJik](https://www.youtube.com/watch?v=esCSpbDPJik)

[^21]:  Andrej Karpathy. Deep Dive into LLMs like ChatGPT. [https://youtu.be/7xTGNNLPyMI](https://youtu.be/7xTGNNLPyMI). Andrej Karpathy. How I use LLMs. [https://youtu.be/EWvNQjAaOHw](https://youtu.be/EWvNQjAaOHw). Andrej Karpathy. \[1hr Talk\] Intro to Large Language Models. [https://www.youtube.com/watch?v=zjkBMFhNj\_g](https://www.youtube.com/watch?v=zjkBMFhNj_g)

[^22]:  Allerdings hinkt der Vergleich an einer entscheidenden Stelle: Im Gegensatz zu einer ZIP-Datei erfolgt die â€œDekompressionâ€ bei LLMs nicht verlustfrei, da es unmÃ¶glich ist, aus dem trainierten Modell sÃ¤mtliche ursprÃ¼nglichen Texte exakt wiederherzustellen. Dennoch halte ich den Vergleich fÃ¼r ein anschauliches und didaktisch sinnvolles Bild.

[^23]:  Modelle, die Sequenzen Token fÃ¼r Token generieren, wobei jedes neue Token nur auf Basis aller vorherigen Token vorhergesagt wird, ohne spÃ¤tere Revision.

[^24]:  Hoffmann, J. et al. (2022). "Training Compute-Optimal Large Language Models". arXiv:2203.15556. Chinchilla-Skalierungsgesetze zeigen optimale VerhÃ¤ltnisse von ModellgrÃ¶ÃŸe zu Trainingsdaten.

[^25]:  Reinforcement Learning (RL) bezeichnet einen Bereich des maschinellen Lernens, bei dem ein Agent durch Ausprobieren lernt, optimale Entscheidungen zu treffen. Der Agent erhÃ¤lt nach jeder Handlung RÃ¼ckmeldungen ("Rewardsâ€œ), die anzeigen, wie gut die Aktion war, und nutzt diese Erfahrungen, um zukÃ¼nftige Entscheidungen so anzupassen, dass die langfristig akkumulierte Belohnung maximiert wird. Es wird dabei keine explizite LÃ¶sung vorgegeben; der Agent lernt allein durch die Interaktion mit seiner Umgebung. Reinforcement Learning \- Computerphile. [https://www.youtube.com/watch?v=844U9T\_SOrA](https://www.youtube.com/watch?v=844U9T_SOrA) 

[^26]:  Reinforcement Learning from Human Feedback (RLHF) Explained. [https://youtu.be/T_X4XFwKX8k](https://youtu.be/T_X4XFwKX8k)

[^27]:  Eine sehr anschauliche Darstellung, die zeigt, wie Transformer funktionieren, gibt es bei 3Blue1Brown. But what is a GPT? Visual intro to transformers | Chapter 5, Deep Learning. [https://www.3blue1brown.com/lessons/gpt](https://www.3blue1brown.com/lessons/gpt) 

[^28]:  Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, und Illia Polosukhin. "Attention Is All You Needâ€œ. v7. arXiv, 2017\. [https://doi.org/10.48550/arXiv.1706.03762](https://doi.org/10.48550/arXiv.1706.03762).

[^29]:  Ethan Mollick. Thinking Like an AI. [https://www.oneusefulthing.org/p/thinking-like-an-ai](https://www.oneusefulthing.org/p/thinking-like-an-ai) 

[^30]:  Ein lokales Optimum ist ein Ergebnis, das im unmittelbaren Umfeld schon das Beste ist â€“ wenn man nur einen kleinen Schritt nach links, rechts, oben oder unten macht, wird es nicht besser. Doch weiter weg kÃ¶nnte es noch ein besseres Ergebnis geben.

[^31]:  Lee Boonstra. Prompt Engineering. [https://www.gptaiflow.tech/assets/files/2025-01-18-pdf-1-TechAI-Google-whitepaper\_Prompt%20Engineering\_v4-af36dcc7a49bb7269a58b1c9b89a8ae1.pdf](https://www.gptaiflow.tech/assets/files/2025-01-18-pdf-1-TechAI-Google-whitepaper_Prompt%20Engineering_v4-af36dcc7a49bb7269a58b1c9b89a8ae1.pdf)

[^32]:  Ilya Sutskever (OpenAI Chief Scientist) \- Why Next-Token Prediction Could Surpass Human Intelligence. [https://youtu.be/Yf1o0TQzry8](https://youtu.be/Yf1o0TQzry8?si=lj8B8UaESaDjTFgM). Why next-token prediction is enough for AGI \- Ilya Sutskever (OpenAI Chief Scientist). [https://youtu.be/YEUclZdj\_Sc](https://youtu.be/YEUclZdj_Sc)

[^33]:  Andrej Karpathy: Software Is Changing (Again). [https://youtu.be/LCEmiRjPEtQ](https://youtu.be/LCEmiRjPEtQ)

[^34]:  Interview mit Amanda Askell. Claudeâ€™s Character. [https://youtu.be/ugvHCXCOmm4](https://youtu.be/ugvHCXCOmm4) 

[^35]:  Shani, Chen, Dan Jurafsky, Yann LeCun, and Ravid Shwartz-Ziv. â€˜From Token to Thoughts: How LLMs and Humans Trade Compression for Meaningâ€™. arXiv, 26 May 2025\. [https://doi.org/10.48550/arXiv.2505.17117](https://doi.org/10.48550/arXiv.2505.17117).

[^36]:  Lindseyâ€ , Authors Jack, Wes Gurnee\*, Emmanuel Ameisen\*, Brian Chen\*, Adam Pearce\*, Nicholas L. Turner\*, Craig Citro\*, et al. â€˜On the Biology of a Large Language Modelâ€™. Transformer Circuits. Accessed 25 May 2025\. [https://transformer-circuits.pub/2025/attribution-graphs/biology.html](https://transformer-circuits.pub/2025/attribution-graphs/biology.html).

[^37]:  Chowdhery et al. (2022) zeigen, dass Googles PaLM-Sprachmodell erst nach einer erheblichen ErhÃ¶hung der Parameterzahl (auf 540 Milliarden) plÃ¶tzlich FÃ¤higkeiten erlangte, wie etwa die LÃ¶sung komplexer logischer und arithmetischer Aufgaben, die kleinere Modelle nicht erreichten. Chowdhery, Aakanksha, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, u. a. "PaLM: Scaling Language Modeling with Pathwaysâ€œ. arXiv, 5\. Oktober 2022\. [https://doi.org/10.48550/arXiv.2204.02311](https://doi.org/10.48550/arXiv.2204.02311).

[^38]:  Wei, Jason, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, u. a. "Emergent Abilities of Large Language Modelsâ€œ. arXiv, 26\. Oktober 2022\. [https://doi.org/10.48550/arXiv.2206.07682](https://doi.org/10.48550/arXiv.2206.07682). 

[^39]:  Schaeffer, Rylan, Brando Miranda, und Sanmi Koyejo. "Are Emergent Abilities of Large Language Models a Mirage?â€œ arXiv, 22\. Mai 2023\. [https://doi.org/10.48550/arXiv.2304.15004](https://doi.org/10.48550/arXiv.2304.15004).

[^40]:  Zhao, Rosie, Tian Qin, David Alvarez-Melis, Sham Kakade, und Naomi Saphra. "Distributional Scaling for Emergent Capabilitiesâ€œ. arXiv, 27\. Mai 2025\. [https://doi.org/10.48550/arXiv.2502.17356](https://doi.org/10.48550/arXiv.2502.17356).

[^41]:  AI Explained. Emergent Behaviors and Grokking. [https://www.coursera.org/learn/8-most-controversial-terms-in-ai-explained/lecture/uo7Y4/emergent-behaviors-and-grokking-part-1](https://www.coursera.org/learn/8-most-controversial-terms-in-ai-explained/lecture/uo7Y4/emergent-behaviors-and-grokking-part-1). Power, Alethea, Yuri Burda, Harri Edwards, Igor Babuschkin, and Vedant Misra. â€˜Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasetsâ€™. Accessed 3 June 2025\. [https://arxiv.org/abs/2201.02177](https://arxiv.org/abs/2201.02177). 

[^42]:  Ruis, Laura, Maximilian Mozes, Juhan Bae, Siddhartha Rao Kamalakara, Dwarak Talupuru, Acyr Locatelli, Robert Kirk, Tim RocktÃ¤schel, Edward Grefenstette, und Max Bartolo. 2025\. "Procedural Knowledge in Pretraining Drives Reasoning in Large Language Modelsâ€œ. arXiv. [https://doi.org/10.48550/arXiv.2411.12580](https://doi.org/10.48550/arXiv.2411.12580).

[^43]:  Shojaee, Parshin, Iman Mirzadeh, Keivan Alizadeh, Maxwell Horton, Samy Bengio, und Mehrdad Farajtabar. "The Illusion of Thinking: Understanding the Strengths and Limitations of Reasoning Models via the Lens of Problem Complexityâ€œ, 2025\. [https://ml-site.cdn-apple.com/papers/the-illusion-of-thinking.pdf](https://ml-site.cdn-apple.com/papers/the-illusion-of-thinking.pdf). FÃ¼r kritische Perspektiven zur Methodik siehe Kevin A. Bryan. [https://x.com/Afinetheorem/status/1931853801293484358](https://x.com/Afinetheorem/status/1931853801293484358) und AI Explained. Appleâ€™s â€˜AI Canâ€™t Reasonâ€™ Claim Seen By 13M+, What You Need to Know. [https://youtu.be/wPBD6wTap7g](https://youtu.be/wPBD6wTap7g)  

[^44]:  FranÃ§ois Chollet. "ARC-AGI: Evaluating Real General Intelligenceâ€œ. [https://arcprize.org/blog/oai-o3-pub-breakthrough](https://arcprize.org/blog/oai-o3-pub-breakthrough)

[^45]:  Subbarao Kambhampati. Diskussion zur Debatte um Reasoning bei LLMs. LinkedIn, 2024\. [https://www.linkedin.com/posts/subbarao-kambhampati-3260708\_ğ—§ğ—µğ—¼ğ˜‚ğ—´ğ—µğ˜ğ˜€-ğ—¼ğ—»-ğ˜ğ—µğ—®ğ˜-ğ—”ğ—½ğ—½ğ—¹ğ—²-activity-7337469667478786048-ODVi](https://www.linkedin.com/posts/subbarao-kambhampati-3260708_ğ—§ğ—µğ—¼ğ˜‚ğ—´ğ—µğ˜ğ˜€-ğ—¼ğ—»-ğ˜ğ—µğ—®ğ˜-ğ—”ğ—½ğ—½ğ—¹ğ—²-activity-7337469667478786048-ODVi)

[^46]:  Chollet, FranÃ§ois. â€œARC-AGI: A Benchmark for General Intelligenceâ€. [https://arcprize.org](https://arcprize.org) 

[^47]:  Chollet et al. (2025): ARC-AGI-2: A New Challenge for Frontier AI Reasoning Systems, arXiv:2505.11831v1 \[cs.AI\], online verfÃ¼gbar unter [https://arxiv.org/abs/2505.11831](https://arxiv.org/abs/2505.11831).

[^48]:  OpenAI. â€œIntroducing OpenAI o3â€. Dezember 2024\. [https://openai.com/index/openai-o3](https://openai.com/index/openai-o3)

[^49]:  AI Wonâ€™t Plateau â€” if We Give It Time To Think | Noam Brown | TED. [https://youtu.be/MG9oqntiJKg](https://youtu.be/MG9oqntiJKg) 

[^50]:  Learning to Reason with LLMs. [https://www.youtube.com/live/Gr_eYXdHFis](https://www.youtube.com/live/Gr_eYXdHFis)

[^51]:  Banerjee, Sourav, Ayushi Agarwal, and Saloni Singla. "LLMs Will Always Hallucinate, and We Need to Live With This.â€œ arXiv, 9\. September 2024\. [https://doi.org/10.48550/arXiv.2409.05746](https://doi.org/10.48550/arXiv.2409.05746). Siehe auch kritische Diskussion: [https://x.com/Afinetheorem/status/1931853801293484358](https://x.com/Afinetheorem/status/1931853801293484358)


[image1]: img/system1-42.png
[image2]: img/tokenizer.png
[image3]: img/dog-cat-stone.png
[image4]: img/embedding.png
[image5]: img/context-window.png
[image6]: img/netxt-token-attention.png