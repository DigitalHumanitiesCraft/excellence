Nachgedacht für 6 Sekunden


# Fisher Corpus Mini-Pipeline

*A lightweight, fully reproducible workflow for investigating two English clause patterns in 50 telephone conversations.*

---

## 1  Project at a Glance

| Aspect               | Details                                                                                                                                                                                                                                                                                   |
| -------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Corpus slice**     | *Fisher Corpus Texts 1–50.xlsx*  (\~8 h spontaneous US conversations).                                                                                                                                                                                                                    |
| **Research targets** | 1. **Dative alternation** <br>  • DO-IO (“*gave Mary a drink*”) <br>  • IO-PP (“*gave a drink to Mary*”) <br>2. **Subject-auxiliary inversion** in seven flavours (negative/restrictive, conditional, degree, comparative, exclamative, optative/formulaic, positive/negative rejoinder). |
| **Outputs**          | • `fisher_clean.parquet` (utterances) <br>• `fisher_clean_merged.parquet` (clauses, gap ≤ 0.5 s) <br>• `dative_sentences.csv` (9 hits) <br>• `inversion_sentences.csv` (83 hits, typed) <br>• QC log & summary files                                                                      |
| **Stack**            | Python 3.11, pandas, spaCy 3.8, PyYAML (optional). No Torch/GPU required.                                                                                                                                                                                                                 |

---

## 2  Folder Layout

```
project_root/
├ data/
│   └─ Fisher Corpus Texts 1-50.xlsx
├ scripts/
│   ├─ prepare_fisher.py   ← data cleaning & merging
│   └─ analyse_fisher.py   ← linguistic pattern search
└ outputs/                 (created at runtime)
    ├─ fisher_clean.parquet
    ├─ fisher_clean_merged.parquet
    ├─ dative_sentences.csv
    ├─ inversion_sentences.csv
    ├─ fisher_summary.csv
    ├─ fisher_schema.yml
    └─ fisher_clean.log
```

(You can run everything from `project_root`; the scripts autodetect paths.)

---

## 3  `prepare_fisher.py`  (rev-D)

| Feature              | Implementation                                                                                              |
| -------------------- | ----------------------------------------------------------------------------------------------------------- |
| **Reading**          | Opens every sheet, Arrow-dtypes if pandas ≥ 2.0.                                                            |
| **Utterance filter** | Regex on “Speaker ID” column → 12 157 utterances.                                                           |
| **Cleaning**         | Strips XML tags, comments, fillers into `text_clean`.                                                       |
| **Merge**            | Adjacent same-speaker turns fused if silence ≤ **`--gap`** sec (default = 0.50 s) → 9 340 clause-like rows. |
| **Artefacts**        | Parquet files, summary CSV, schema YAML, dual-sink log.                                                     |
| **CLI**              | `python prepare_fisher.py [xlsx] [--gap 0.4]`                                                               |

---

## 4  `analyse_fisher.py`  (rev-D)

| Stage                  | What happens                                                                                                                                     |
| ---------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------ |
| **Parse**              | Loads `fisher_clean_merged.parquet`, processes `text_clean` with spaCy (`en_core_web_sm`, CPU).                                                  |
| **Dative detector**    | Searches for verbs in a 24-item alternator list plus dependency patterns: <br>  • OBJ + IOBJ / OBL-to → DO-IO <br>  • OBJ + prep-to pobj → IO-PP |
| **Inversion detector** | Determines `inv_type` for each clause:  <br>  NEG, COND, DEGREE, COMP, EXCL, OPT, REJ  via lexical triggers and AUX-before-subject test.         |
| **Exports**            | Two CSVs with sentence-level metadata.                                                                                                           |
| **Console summary**    | Frequency table for dative types and inversion types.                                                                                            |
| **CLI**                | `python analyse_fisher.py [parquet]` (default path ok).                                                                                          |

---

## 5  Key Results (50-text slice)

| Pattern           | Hits   |
| ----------------- | ------ |
| IO-PP dative      | **9**  |
| DO-IO dative      | 0      |
| Inversion: DEGREE | **43** |
| EXCL              | 33     |
| COMP              | 3      |
| OPT / formulaic   | 3      |
| COND              | 1      |
| NEG (strict list) | 0      |

*Interpretation*: spontaneous phone speech favours *to-PP* over double-object datives; degree and exclamative inversions are the only lively SA-inversion subtypes in this genre.

---

## 6  How to Extend

1. **Broader corpus** – Point `prepare_fisher.py` at additional Fisher workbooks, or replace with COCA/COHA TSVs after minor column mapping.
2. **Custom merge window** – Tune `--gap`; 0.3 s is stricter, 0.7 s merges more hesitations.
3. **Heuristic tweaks** – The inversion detector uses simple regex/dep checks; adjust trigger sets or add a spaCy `DependencyMatcher` pattern for finer control.
4. **Stat modelling** – Load the Parquet in R or Python’s `statsmodels` for mixed-effects analysis (`conv_id` as random intercept).

---

## 7  Quick-start Commands

```bash
# 1  Clean & merge (0.5 s gap)
python scripts/prepare_fisher.py

# 2  Run linguistic search
python scripts/analyse_fisher.py

# 3  Inspect
open outputs/dative_sentences.csv
open outputs/inversion_sentences.csv
```

That’s it—**one Excel file → four reusable data products → publishable numbers** in under five minutes on any laptop.
